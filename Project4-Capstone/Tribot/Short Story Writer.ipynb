{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow..\n",
    "Next word prediction after n_input words learned from text file.\n",
    "A story is automatically generated if the predicted word is fed back as input.\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target log path\n",
    "logs_path = '/tmp/tensorflow/rnn_words'\n",
    "writer = tf.summary.FileWriter(logs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n",
      "[\"Macy's,\", 'Inc.', 'is', 'an', 'omnichannel', 'retail', 'company', 'operating', 'stores,', 'Websites', 'and', 'mobile', 'applications', 'under', 'various', 'brands,', 'such', 'as', \"Macy's,\", \"Bloomingdale's\", 'and', 'Bluemercury.', 'The', 'Company', 'sells', 'a', 'range', 'of', 'merchandise,', 'including', 'apparel', 'and', 'accessories', \"(men's,\", \"women's\", 'and', \"children's),\", 'cosmetics,', 'home', 'furnishings', 'and', 'other', 'consumer', 'goods.', 'Its', 'subsidiaries', 'provide', 'various', 'support', 'functions', 'to', 'its', 'retail', 'operations.', 'Its', 'bank', 'subsidiary,', 'FDS', 'Bank,', 'provides', 'credit', 'processing,', 'certain', 'collections,', 'customer', 'service', 'and', 'credit', 'marketing', 'services', 'in', 'respect', 'of', 'all', 'credit', 'card', 'accounts', 'that', 'are', 'owned', 'either', 'by', 'Department', 'Stores', 'National', 'Bank', '(DSNB),', 'which', 'is', 'a', 'subsidiary', 'of', 'Citibank', 'N.A.,', 'or', 'FDS', 'Bank.', 'The', 'private', 'label', 'brands', 'offered', 'by', 'the', 'Company', 'include', 'Alfani,', 'American', 'Rag,', 'Aqua,', 'Bar', 'III,', 'Belgique,', 'Charter', 'Club,', 'Club', 'Room,', 'Epic', 'Threads,', 'first', 'impressions,', 'Giani', 'Bernini,', 'Greg', 'Norman', 'for', 'Tasso', 'Elba,', 'Holiday', 'Lane,', 'Home', 'Design,', 'Hotel', 'Collection,', 'John', 'Ashford,', 'Karen', 'Scott,', 'Thalia', 'Sodi', 'and', 'lune+aster.']\n"
     ]
    }
   ],
   "source": [
    "# Text file containing words for training\n",
    "training_file = 'macys.txt'\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "content_list = []\n",
    "training_data = [item for sub_list in training_data for item in sub_list]\n",
    "\n",
    "print(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 2-layer LSTM, each layer has n_hidden units.\n",
    "    # Average Accuracy= 95.20% at 50k iter\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "    # Average Accuracy= 90.60% 50k iter\n",
    "    # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above\n",
    "    # rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1000, Average Loss= 4.218331, Average Accuracy= 9.80%\n",
      "['Karen', 'Scott,', 'Thalia'] - [Sodi] vs [Ashford,]\n",
      "Iter= 2000, Average Loss= 2.144607, Average Accuracy= 44.70%\n",
      "['or', 'FDS', 'Bank.'] - [The] vs [operating]\n",
      "Iter= 3000, Average Loss= 1.897968, Average Accuracy= 57.30%\n",
      "['services', 'in', 'respect'] - [of] vs [Aqua,]\n",
      "Iter= 4000, Average Loss= 1.779946, Average Accuracy= 59.20%\n",
      "['in', 'respect', 'of'] - [all] vs [all]\n",
      "Iter= 5000, Average Loss= 1.323889, Average Accuracy= 68.70%\n",
      "['support', 'functions', 'to'] - [its] vs [III,]\n",
      "Iter= 6000, Average Loss= 1.369421, Average Accuracy= 69.10%\n",
      "['cosmetics,', 'home', 'furnishings'] - [and] vs [is]\n",
      "Iter= 7000, Average Loss= 0.927873, Average Accuracy= 78.40%\n",
      "['applications', 'under', 'various'] - [brands,] vs [brands,]\n",
      "Iter= 8000, Average Loss= 0.852452, Average Accuracy= 80.20%\n",
      "['under', 'various', 'brands,'] - [such] vs [Its]\n",
      "Iter= 9000, Average Loss= 0.887253, Average Accuracy= 78.10%\n",
      "['company', 'operating', 'stores,'] - [Websites] vs [Websites]\n",
      "Iter= 10000, Average Loss= 0.840709, Average Accuracy= 80.90%\n",
      "['is', 'an', 'omnichannel'] - [retail] vs [retail]\n",
      "Iter= 11000, Average Loss= 0.679976, Average Accuracy= 85.50%\n",
      "['impressions,', 'Giani', 'Bernini,'] - [Greg] vs [Greg]\n",
      "Iter= 12000, Average Loss= 0.651350, Average Accuracy= 84.00%\n",
      "['that', 'are', 'owned'] - [either] vs [Elba,]\n",
      "Iter= 13000, Average Loss= 0.666732, Average Accuracy= 84.70%\n",
      "['that', 'are', 'owned'] - [either] vs [Elba,]\n",
      "Iter= 14000, Average Loss= 0.656981, Average Accuracy= 84.20%\n",
      "['all', 'credit', 'card'] - [accounts] vs [accounts]\n",
      "Iter= 15000, Average Loss= 0.549806, Average Accuracy= 86.90%\n",
      "['various', 'support', 'functions'] - [to] vs [to]\n",
      "Iter= 16000, Average Loss= 0.478319, Average Accuracy= 89.30%\n",
      "['apparel', 'and', 'accessories'] - [(men's,] vs [(men's,]\n",
      "Iter= 17000, Average Loss= 0.487732, Average Accuracy= 89.70%\n",
      "['The', 'Company', 'sells'] - [a] vs [a]\n",
      "Iter= 18000, Average Loss= 0.469373, Average Accuracy= 89.80%\n",
      "['omnichannel', 'retail', 'company'] - [operating] vs [operating]\n",
      "Iter= 19000, Average Loss= 0.597391, Average Accuracy= 86.50%\n",
      "['Norman', 'for', 'Tasso'] - [Elba,] vs [Elba,]\n",
      "Iter= 20000, Average Loss= 0.531241, Average Accuracy= 86.70%\n",
      "['Aqua,', 'Bar', 'III,'] - [Belgique,] vs [Belgique,]\n",
      "Iter= 21000, Average Loss= 0.422957, Average Accuracy= 90.90%\n",
      "['National', 'Bank', '(DSNB),'] - [which] vs [which]\n",
      "Iter= 22000, Average Loss= 0.374293, Average Accuracy= 91.30%\n",
      "['card', 'accounts', 'that'] - [are] vs [are]\n",
      "Iter= 23000, Average Loss= 0.353203, Average Accuracy= 92.10%\n",
      "['either', 'by', 'Department'] - [Stores] vs [Stores]\n",
      "Iter= 24000, Average Loss= 0.463932, Average Accuracy= 89.60%\n",
      "['in', 'respect', 'of'] - [all] vs [all]\n",
      "Iter= 25000, Average Loss= 0.328212, Average Accuracy= 93.20%\n",
      "['services', 'in', 'respect'] - [of] vs [of]\n",
      "Iter= 26000, Average Loss= 0.377017, Average Accuracy= 91.80%\n",
      "['subsidiaries', 'provide', 'various'] - [support] vs [support]\n",
      "Iter= 27000, Average Loss= 0.396057, Average Accuracy= 92.50%\n",
      "['other', 'consumer', 'goods.'] - [Its] vs [Its]\n",
      "Iter= 28000, Average Loss= 0.345515, Average Accuracy= 92.30%\n",
      "['home', 'furnishings', 'and'] - [other] vs [other]\n",
      "Iter= 29000, Average Loss= 0.272414, Average Accuracy= 94.10%\n",
      "['consumer', 'goods.', 'Its'] - [subsidiaries] vs [subsidiaries]\n",
      "Iter= 30000, Average Loss= 0.312857, Average Accuracy= 92.30%\n",
      "['such', 'as', \"Macy's,\"] - [Bloomingdale's] vs [Bloomingdale's]\n",
      "Iter= 31000, Average Loss= 0.278115, Average Accuracy= 95.00%\n",
      "['omnichannel', 'retail', 'company'] - [operating] vs [operating]\n",
      "Iter= 32000, Average Loss= 0.316247, Average Accuracy= 93.60%\n",
      "['Holiday', 'Lane,', 'Home'] - [Design,] vs [Design,]\n",
      "Iter= 33000, Average Loss= 0.281116, Average Accuracy= 93.70%\n",
      "['The', 'private', 'label'] - [brands] vs [FDS]\n",
      "Iter= 34000, Average Loss= 0.299623, Average Accuracy= 94.80%\n",
      "['are', 'owned', 'either'] - [by] vs [by]\n",
      "Iter= 35000, Average Loss= 0.288222, Average Accuracy= 93.30%\n",
      "['certain', 'collections,', 'customer'] - [service] vs [service]\n",
      "Iter= 36000, Average Loss= 0.246781, Average Accuracy= 94.30%\n",
      "['cosmetics,', 'home', 'furnishings'] - [and] vs [and]\n",
      "Iter= 37000, Average Loss= 0.227276, Average Accuracy= 95.00%\n",
      "['Bluemercury.', 'The', 'Company'] - [sells] vs [sells]\n",
      "Iter= 38000, Average Loss= 0.308035, Average Accuracy= 93.10%\n",
      "['range', 'of', 'merchandise,'] - [including] vs [including]\n",
      "Iter= 39000, Average Loss= 0.309906, Average Accuracy= 93.10%\n",
      "['Karen', 'Scott,', 'Thalia'] - [Sodi] vs [Sodi]\n",
      "Iter= 40000, Average Loss= 0.255187, Average Accuracy= 94.00%\n",
      "['Club,', 'Club', 'Room,'] - [Epic] vs [Epic]\n",
      "Iter= 41000, Average Loss= 0.287987, Average Accuracy= 93.90%\n",
      "['label', 'brands', 'offered'] - [by] vs [by]\n",
      "Iter= 42000, Average Loss= 0.328029, Average Accuracy= 95.10%\n",
      "['is', 'a', 'subsidiary'] - [of] vs [of]\n",
      "Iter= 43000, Average Loss= 0.295109, Average Accuracy= 94.60%\n",
      "['respect', 'of', 'all'] - [credit] vs [credit]\n",
      "Iter= 44000, Average Loss= 0.266797, Average Accuracy= 94.20%\n",
      "['provides', 'credit', 'processing,'] - [certain] vs [certain]\n",
      "Iter= 45000, Average Loss= 0.235881, Average Accuracy= 94.80%\n",
      "['support', 'functions', 'to'] - [its] vs [its]\n",
      "Iter= 46000, Average Loss= 0.075091, Average Accuracy= 98.20%\n",
      "['accessories', \"(men's,\", \"women's\"] - [and] vs [and]\n",
      "Iter= 47000, Average Loss= 0.262017, Average Accuracy= 95.00%\n",
      "['Company', 'sells', 'a'] - [range] vs [range]\n",
      "Iter= 48000, Average Loss= 0.233631, Average Accuracy= 95.30%\n",
      "['an', 'omnichannel', 'retail'] - [company] vs [company]\n",
      "Iter= 49000, Average Loss= 0.126630, Average Accuracy= 97.70%\n",
      "['Bar', 'III,', 'Belgique,'] - [Charter] vs [Charter]\n",
      "Iter= 50000, Average Loss= 0.204800, Average Accuracy= 96.10%\n",
      "['label', 'brands', 'offered'] - [by] vs [for]\n",
      "Optimization Finished!\n",
      "Elapsed time:  38.6826125503 min\n",
      "Run on command line.\n",
      "\ttensorboard --logdir=/tmp/tensorflow/rnn_words\n",
      "Point your web browser to: http://localhost:6006/\n",
      "3 words:  \"Macy's and Bloomingdale's\"\n",
      "Word not in dictionary\n",
      "3 words: 'label and brands'\n",
      "label and brands mobile under various brands, such to its retail operations. Its bank subsidiary, brands offered by the Company include Alfani, American Rag, Aqua, Bar III, Belgique, by Department Stores National Bank (DSNB), which\n",
      "3 words: \"Macy's, omnichannel retail\"\n",
      "Macy's, omnichannel retail an operating stores, Websites and mobile applications under various brands, such to its retail operations. Its bank subsidiary, brands offered by the Company include Alfani, American Rag, Aqua, Bar III, Belgique, by\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    while step < training_iters:\n",
    "        # Generate a minibatch. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = training_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    print(\"Run on command line.\")\n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"Point your web browser to: http://localhost:6006/\")\n",
    "    while True:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "#         sentence = str(sentence)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
